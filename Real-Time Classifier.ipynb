{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db4a48e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\muner\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "C:\\Users\\muner\\anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py:83: UserWarning: CUDA initialization: CUDA driver initialization failed, you might not have a CUDA gpu. (Triggered internally at  C:\\cb\\pytorch_1000000000000\\work\\c10\\cuda\\CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import bs4 as bs4\n",
    "from urllib.parse import urlparse\n",
    "import requests\n",
    "import nltk \n",
    "nltk.download('words')\n",
    "import re\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "import joblib\n",
    "pipeline = joblib.load('NLP_pipline.sav')\n",
    "classes=['Adult','Business/Corporate', 'Computers and Technology','E-Commerce','Education','Food','Forums','Games',\n",
    "         'Health and Fitness','Law and Government','News', 'Photography', 'Social Networking and Messaging','Sports',\n",
    "         'Streaming Services', 'Travel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecf4d981",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_content(soup):\n",
    "    '''returns the text content of the whole page with some exception to tags. See tags_to_ignore.'''\n",
    "    tags_to_ignore = ['style', 'script', 'head', 'title', 'meta', '[document]',\"h1\",\"h2\",\"h3\",\"h4\",\"h5\",\"h6\",\"noscript\"]\n",
    "    tags = soup.find_all(text=True)\n",
    "    result = []\n",
    "    for tag in tags:\n",
    "        stripped_tag = tag.strip()\n",
    "        if tag.parent.name not in tags_to_ignore\\\n",
    "            and isinstance(tag, bs4.element.Comment)==False\\\n",
    "            and not stripped_tag.isnumeric()\\\n",
    "            and len(stripped_tag)>0:\n",
    "            result.append(stripped_tag)\n",
    "    return ' '.join(result)\n",
    "    \n",
    "def scraping(URL):\n",
    "    content = requests.get(URL,timeout=60).content\n",
    "    soup = BeautifulSoup(content, \"lxml\")\n",
    "    metatags = soup.find_all(lambda tag: (tag.name==\"meta\") & (tag.has_attr('name') & (tag.has_attr('content'))))\n",
    "    metacontent = [str(tag[\"content\"]) for tag in metatags if tag[\"name\"] in ['keywords','description']]\n",
    "    \n",
    "    headtags = soup.find_all([\"h1\",\"h2\",\"h3\",\"h4\",\"h5\",\"h6\"])\n",
    "    headcontent = [\" \".join(tag.stripped_strings) for tag in headtags]\n",
    "    result = {\n",
    "            \"website_url\": URL,\n",
    "            \"website_name\": \"\".join(urlparse(URL).netloc.split(\".\")[-2]),\n",
    "            \"website_text\": '. '.join(soup.title.contents)+' '.join(metacontent)+' '.join(headcontent)+get_text_content(soup)}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fdf6321",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(doc):\n",
    "    '''\n",
    "    Clean the document. Remove pronouns, stopwords, lemmatize the words and lowercase them\n",
    "    '''\n",
    "    words = set(nltk.corpus.words.words())\n",
    "    doc = nlp(doc)\n",
    "    tokens = []\n",
    "    exclusion_list = [\"nan\"]\n",
    "    for token in doc:\n",
    "        if token.is_stop or token.is_punct or token.text.isnumeric() or (token.text.isalnum()==False) or token.text in exclusion_list :\n",
    "            continue\n",
    "        \n",
    "        token = str(token.lemma_.lower().strip())\n",
    "        tokens.append(token)\n",
    "        \n",
    "    text=\" \".join(tokens) \n",
    "    text=re.sub(\" +\", \" \", text)\n",
    "    text= [w for w in nltk.wordpunct_tokenize(text) if w.lower() in words or not w.isalpha()]\n",
    "    return \" \".join(text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "500eb28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beautiful_predict(URL):\n",
    "    r=scraping(URL)\n",
    "    text=r[\"website_text\"]\n",
    "    text=clean_text(text)\n",
    "    p=pipeline.predict([text])\n",
    "    print('The website category is:',classes[p[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a525cd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def edu_blocked(URL):\n",
    "    r=scraping(URL)\n",
    "    text=r[\"website_text\"]\n",
    "    text=clean_text(text)\n",
    "    p=pipeline.predict([text])\n",
    "    if p[0]==0 or p[0]==3 or p[0]==6 or p[0]==7 or p[0]==12 or p[0]==14 or p[0]==15:\n",
    "        return 1\n",
    "    else: return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8a2a8b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The website category is: Computers and Technology\n"
     ]
    }
   ],
   "source": [
    "beautiful_predict('https://openai.com/blog/chatgpt/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6925b131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The website category is: Health and Fitness\n"
     ]
    }
   ],
   "source": [
    "beautiful_predict('https://www.who.int/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81e5e6c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The website category is: Education\n"
     ]
    }
   ],
   "source": [
    "beautiful_predict('https://www.iau.edu.sa/en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb366b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The website category is: Computers and Technology\n"
     ]
    }
   ],
   "source": [
    "beautiful_predict('https://scikit-learn.org/stable/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea6b81c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The website category is: Computers and Technology\n"
     ]
    }
   ],
   "source": [
    "beautiful_predict('https://github.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0642fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The website category is: Food\n"
     ]
    }
   ],
   "source": [
    "beautiful_predict('https://www.allrecipes.com/recipes/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f600d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The website category is: Forums\n"
     ]
    }
   ],
   "source": [
    "beautiful_predict('https://www.reddit.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e577ea6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The website category is: Law and Government\n"
     ]
    }
   ],
   "source": [
    "beautiful_predict('https://www.moh.gov.sa/en/Pages/default.aspx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402ceacf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
